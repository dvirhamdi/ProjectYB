import numpy as np
#import HD
import pickle
import os


class Danse():
    def __init__(self, n, inputs,activtion_function = 'linear'):

        self.inputs = 0
        self.weights = 0.5 * np.random.random((inputs, n)) - 1
        self.biass = np.zeros((n))

        self.set_acti(activtion_function.lower())

    def set_acti(self,activtion_function):

        if activtion_function == 'sigmoid':
            self.activtion_function = self.sigmoid
            self.d_activtion_function = self.sigmoid_derivative

        elif activtion_function == 'soft_max':
            self.activtion_function = self.soft_max
            self.d_activtion_function = self.soft_max_derivative

        elif activtion_function == 'linear':
            self.activtion_function = self.linear
            self.d_activtion_function = self.linear_derivative

    def setW(self, W):
        if self.weights.shape == W.shape:
            self.weights = W
        else:
            raise ValueError('shapes must be the same!')

    # calculate the softmax of a vector
    def soft_max(self,x):
        x = x - np.max(x)
        #print(x,'xxx')
        e = np.exp(x)
        #print(e,'eee')
        d = np.sum(e,axis=1)
        #print(d,'ddd')
        g = np.array(np.split(d,list(range(0,d.shape[0])))[1:])
        #g = np.clip(g,np.min(g)+1e-10,np.max(g)+1e-10)
        #print(e/g,'ggg')
        return e / g


    def sm_dir(self,S):
        S = S[0]
        S_vector = S.reshape(S.shape[0],1)
        S_matrix = np.tile(S_vector,S.shape[0])
        S_dir = np.diag(S) - (S_matrix * np.transpose(S_matrix))
        return S_dir

    def z_dir(self,Z, W, x):
        dir_matrix = np.zeros((W.shape[0] * W.shape[1], Z.shape[0]))

        for k in range(0, Z.shape[0]):
            for i in range(0, W.shape[1]):
                for j in range(0, W.shape[0]):
                    if i == k:
                        dir_matrix[(i*W.shape[0]) + j][k] = x[0][j]

        return dir_matrix

    def soft_max_derivative(self,x,z):

        der = []

        for i in range(x.shape[0]):
            DS = self.sm_dir(self.soft_max([x[i]]))
            DZ = self.z_dir(z,self.weights,x)
            DL = np.dot(DS,np.transpose(DZ))
            der.append(DL)
        print(der)
        print(self.weights.shape)
        return np.array(der)


    def linear(self,x):
        return x


    def linear_derivative(self,x,parm = 0):
        return x*0 + 1

    # The Sigmoid function, which describes an S shaped curve.
    # We pass the weighted sum of the inputs through this function to
    # normalise them between 0 and 1.
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))


    def sigmoid_derivative(self,x,parm = 0):
        pass

    # The neural network thinks.
    def think(self, inputs):
        self.inputs = inputs
        self.output = self.activtion_function(np.dot(inputs, self.weights) + self.biass)
        # Pass inputs through our neural network (our single neuron).
        return self.output


class Model():
    def __init__(self):
        self.layers = []
        self.loss = None
        self.d_loss = None

    def add(self, layer):
        self.layers.append(layer)

    def train(self, x, y, epochs=100):
        for e in range(epochs):
            #print(x.shape)
            output = self.predict(x)
            # output = np.clip(output,1e-5, 1 - 1e-5)
            print(f'epochs:{e},loss:{self.loss(pred=output, true=y)}')

            for dlay in range(len(self.layers)):
                lay = self.layers[dlay]
                #print(lay.weights.shape,'\n')
                #print(lay.inputs.shape,'\n')
                #print(lay.weights)
                z = np.dot(lay.inputs,lay.weights)
                #print('z',z.shape,'\n')
                z = z + lay.biass

                res = np.dot(lay.inputs,lay.d_activtion_function(x = lay.inputs,z = z))
                print(res.shape)
                #print(lay.d_activtion_function(z))
                #print('resss',res)
                da = (self.d_loss(output,y) * 0 + 1).T
                #print(output)

                for i,tay in enumerate(self.layers[dlay+1:]):
                    #print('W',tay.weights.shape,'\n')
                    #print('i',tay.inputs.shape,'\n')
                    z = np.dot(tay.weights.T ,tay.inputs.T)
                    #print('z',z.shape,'\n')

                    da = tay.d_activtion_function(z)
                    print('ddd')

                if len(self.layers) != 1:
                    raise
                    da = np.sum(np.dot(da,self.d_loss(output,y)))
                    print(res.shape,'res')
                    res = np.dot(res,da)

                #print('res',res)
                lay.weights -= 0.001 * res
                #lay.biass -= 0.01 * (lay.d_activtion_function(lay.inputs,'B'))


    def mse(self, pred, true):
        return np.mean((pred - true) ** 2)


    def mse_derivative(self, pred,true):
        pass


    def CrossEntropy(self,pred, true):
        #pred = np.clip(pred,1e-10,1 - 1e-10)
        # calculate cross entropy
        return -np.sum([true[i] * np.log(pred[i]) for i in range(len(true))])


    def CrossEntropy_derivative(self,pred,true):
        # Number of samples
        pred = np.clip(pred,np.min(pred)+1e-50,np.max(pred)+1e-50)

        samples = len(pred)
        # Number of labels in every sample
        # We'll use the first sample to count them
        labels = len(pred[0])
        # If labels are sparse, turn them into one-hot vector
        y_true = true
        if len(true.shape) == 1:
            y_true = np.eye(labels)[true]
        # Calculate gradient
        dinputs = -y_true / pred
        # Normalize gradient
        dinputs = dinputs / samples
        return dinputs


    def compile(self,loss):
        if loss == 'mse':
            self.loss = self.mse
            self.d_loss = self.mse_derivative

        if loss == 'crossEntropy':
            self.loss = self.CrossEntropy
            self.d_loss = self.CrossEntropy_derivative


    def predict(self, x):
        t = x
        for lay in self.layers:
            t = lay.think(t)

        return t


    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))


    def sigmoid_derivative(self, x):
        return self.sigmoid(x) * (1 - self.sigmoid(x))

    # calculate the softmax of a vector
    def soft_max(self,x):
        e = np.exp(x)
        d = np.sum(e,axis=1)
        g = np.array(np.split(d,list(range(0,d.shape[0])))[1:])
        return e / g


    def softmax_derivative(self,x):
        SM = x.reshape((-1, 1))
        jac = np.diagflat(x) - np.dot(SM, SM.T)


    def evaluate(self, X_test, y_test):
        pred = self.predict(X_test)
        pred = np.round(pred)
        score = np.argwhere(np.all(pred == y_test,axis=1))
        return len(score) / len(y_test)


    def save(self, path):
        W = []
        for lay in self.layers:
            W.append(lay.weights)

        pickle.dump(W, open(path + '.p', 'wb'))


    def load(self, path):
        W = pickle.load(open(path, 'rb'))
        for i, w in enumerate(W):
            self.layers[i].setW(w)


X =  np.array([[0,1,1],
               [1,1,1],
               [0,1,0]])

y = np.array([[1,0],[0,1],[1,0]])


layer1 = Danse(n=2, inputs=3,activtion_function = 'soft_max')
#layer2 = Danse(n=2, inputs=2,activtion_function = 'soft_max')
model = Model()


model.add(layer1)
#model.add(layer2)

#X_train, X_test, y_train, y_test = HD.get_data(r'D:\cyber\yb project\databases\photos')
# X_train,X_test = HD.scale_data(X_train,X_test)


model.compile(loss='crossEntropy')
model.train(X, y, epochs=100000)

print('model trained!')
#os.chdir(r'C:\Users\Dvir hamdi\PycharmProjects\cyberHW\yodbet project')
print('pred',model.predict(X))

model.save('model')
model.load('model.p')
# Test the neural network with a new situations.
print('acc:', model.evaluate(X, y) * 100, '%')
