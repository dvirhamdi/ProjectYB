import time

import numpy as np
import HD
import pickle
import matplotlib.pyplot as plt
import os

#np.random.seed(0)

class Danse():
    def __init__(self, n, inputs,activtion_function = 'linear'):

        self.inputs = 0
        self.z = 0 # output of the layer before activtion function
        self.weights = 1 * np.random.randn(inputs, n) / np.sqrt(inputs)
        print(np.max(self.weights))
        self.biass = np.zeros((n))


        self.set_acti(activtion_function.lower())

    def set_acti(self,activtion_function):

        if activtion_function == 'sigmoid':
            self.activtion_function = self.sigmoid
            self.d_activtion_function = self.sigmoid_derivative

        elif activtion_function == 'soft_max':
            self.activtion_function = self.soft_max
            self.d_activtion_function = self.soft_max_derivative

        elif activtion_function == 'linear':
            self.activtion_function = self.linear
            self.d_activtion_function = self.linear_derivative

        elif activtion_function == 'relu':
            self.activtion_function = self.relu
            self.d_activtion_function = self.relu_derivative

        elif activtion_function == 'tanh':
            self.activtion_function = self.tanh
            self.d_activtion_function = self.tanh_derivative


    def get_z(self):
        return self.z

    def setW(self, W):
        if self.weights.shape == W.shape:
            self.weights = W
        else:
            raise ValueError('shapes must be the same!')

    def setb(self, b):
        if self.biass.shape == b.shape:
            self.biass = b
        else:
            raise ValueError('shapes must be the same!')

    def tanh(self,x):
        return np.tanh(x)

    def tanh_derivative(self,x):
        return 1 - self.tanh(x)

    def relu(self,x):
        return x * (x >= 0)

    def relu_derivative(self,x,pram = 0):
        return np.maximum(0,x)

    # calculate the softmax of a vector
    def soft_max(self,x):
        #print('x',x)
        x -= np.max(x)
        exp_scores = np.exp(x)
        softmax = exp_scores / np.sum(exp_scores,
                                      axis = 1, keepdims = True)

        return softmax


    def soft_max_derivative(self,x,y,parm = 0):
        #http://saitcelebi.com/tut/output/part2.html
        #x is the input

        a = self.soft_max(x.T) # output

        #print(a.shape)
        #print(x.shape)
        da = (-y / a)
        dz = 0

        k = np.shape(a)[1]
        matrix_l = []
        for i in range(np.shape(a)[0]):
            matrix = np.dot(np.array([a[i]]).T, np.ones((1, k))) * (np.identity(k) - np.dot(np.ones((k, 1)),
                                                                                            np.array([a[i]])))
            dz = np.dot(matrix, da[i])

            matrix_l.append(dz)

        #print(matrix_l)
        matrix_l = np.array(matrix_l)
        #print(matrix_l)
        #print(matrix_l.shape)
        #print('ddd')
        return matrix_l

    def linear(self,x):
        return x*1


    def linear_derivative(self,x,parm = 0):
        return x*0 + 1

    # The Sigmoid function, which describes an S shaped curve.
    # We pass the weighted sum of the inputs through this function to
    # normalise them between 0 and 1.
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))


    def sigmoid_derivative(self,x,parm = 0):
        pass

    # The neural network thinks.
    def think(self, inputs):
        self.inputs = inputs
        self.z = np.dot(inputs, self.weights) + self.biass
        self.output = self.activtion_function(self.z)
        # Pass inputs through our neural network (our single neuron).
        return self.output


class Model():
    def __init__(self):
        self.layers = []
        self.loss = None
        self.d_loss = None

        self.lr = 0.01

    def add(self, layer):
        self.layers.append(layer)

    def train(self, x, y, epochs=100):
        end = 0
        start = 0

        for e in range(epochs):
            #print(x.shape)
            output = self.predict(x)
            #print(output)
            # output = np.clip(output,1e-5, 1 - 1e-5)
            end = time.time()
            if e %25 == 0:
                print(f'epochs:{e},loss:{self.loss(pred=output, true=y)},time took{round(start-end,2)} sec')
                start = time.time()

            W1 = self.layers[0].weights
            b1 = self.layers[0].biass
            W2 = self.layers[1].weights
            b2 = self.layers[1].biass

            z1 = self.layers[0].z
            a1 = self.layers[1].inputs
            delta3 = np.copy(output)
            delta3[range(x.shape[0]),y] -= 1
            dW2 = a1.T.dot(delta3)
            db2 = np.sum(delta3, axis=0, keepdims=False)
            delta2 = delta3.dot(W2.T) * self.layers[0].d_activtion_function(z1)
            dW1 = np.dot(x.T,delta2)
            db1 = np.sum(delta2, axis=0 ,keepdims=False)

            #print(dW1)

            W1 -= dW1 * self.lr
            b1 -= db1 * self.lr
            W2 -= dW2 * self.lr
            b2 -= db2 * self.lr

            self.layers[0].setW(W1)
            self.layers[0].setb(b1)

            self.layers[1].setW(W2)
            self.layers[1].setb(b2)



    def mse(self, pred, true):
        return np.mean((true - pred) ** 2)


    def mse_derivative(self, pred,true):
        #TODO: add derivative by biass!
        return -2*(true - pred)


    def CrossEntropy(self,pred, true):
        m = np.zeros(true.shape[0])
        for i,correct_index in enumerate(true):
            predicted = pred[i][correct_index]
            m[i] = predicted
        log_prob = -np.log(m)
        loss = np.sum(log_prob)

        return float(loss / true.shape[0])


    def CrossEntropy_derivative(self,pred,true):

        # Number of samples
        pred = np.clip(pred,np.min(pred)+1e-50,np.max(pred)+1e-50)

        samples = len(pred)
        # Number of labels in every sample
        # We'll use the first sample to count them
        labels = len(pred[0])
        # If labels are sparse, turn them into one-hot vector
        y_true = true
        if len(true.shape) == 1:
            y_true = np.eye(labels)[true]
        # Calculate gradient
        dinputs = -y_true / pred
        # Normalize gradient
        dinputs = dinputs / samples
        return dinputs



    def compile(self,loss, lr):
        self.lr = lr

        if loss == 'mse':
            self.loss = self.mse
            self.d_loss = self.mse_derivative

        if loss == 'crossEntropy':
            self.loss = self.CrossEntropy
            self.d_loss = self.CrossEntropy_derivative


    def predict(self, x):
        t = x
        for lay in self.layers:
            t = lay.think(t)

        return t


    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))


    def sigmoid_derivative(self, x):
        return self.sigmoid(x) * (1 - self.sigmoid(x))

    # calculate the softmax of a vector
    def soft_max(self,x):
        e = np.exp(x)
        d = np.sum(e,axis=1)
        g = np.array(np.split(d,list(range(0,d.shape[0])))[1:])
        raise '????'
        #return e / g



    def evaluate(self, X_test, y_test):
        pred = self.predict(X_test)
        pred = np.argmax(pred,axis=1)
        print(pred)
        print(y_test[0])
        score = np.argwhere(pred == y_test[0])
        return score.shape[0] / y_test.shape[1]


    def save(self, path):
        W = []
        for lay in self.layers:
            W.append(lay.weights)

        pickle.dump(W, open(path + '.p', 'wb'))


    def load(self, path):
        W = pickle.load(open(path, 'rb'))
        for i, w in enumerate(W):
            self.layers[i].setW(w)


X_train =  np.array([[0,1,1],
               [1,1,1],
               [0,1,0],
               [1,0,1]])

y_train = np.array([0,1,0,1])
print(y_train.shape,'y')

layer1 = Danse(n=128, inputs=16384,activtion_function = 'tanh')
layer2 = Danse(n=3, inputs=128,activtion_function = 'soft_max')
model = Model()


model.add(layer1)
model.add(layer2)

X_train, X_test, y_train, y_test = HD.get_data(r'D:\cyber\yb project\databases\photos')
X_train,X_test = HD.scale_data(X_train,X_test)

print(y_train.shape)
print(np.max(X_train))
print(y_train)
y_train = y_train[0]

print('data loaded!')
print(X_test[0].shape)
t = np.reshape(X_test[0],(128,128))
#t *= 255

print(t)
plt.imshow(t)
plt.show()

model.compile(loss='crossEntropy',lr = 0.0001)
model.train(X_train, y_train, epochs=100)

print('model trained!')
#os.chdir(r'C:\Users\Dvir hamdi\PycharmProjects\cyberHW\yodbet project')
print('pred',model.predict(X_test))

#model.save('model')
#model.load('model.p')
# Test the neural network with a new situations.
print('acc:', model.evaluate(X_test, y_test) * 100, '%')

print(y_test[0])
plt.imshow(np.reshape(X_test[0],(128,128)))
plt.show()

