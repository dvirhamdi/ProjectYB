import time

import numpy as np
import HD
import pickle
import matplotlib.pyplot as plt
import os
from numba import jit, cuda


#np.random.seed(0)

class Danse():
    def __init__(self, n, inputs,activtion_function = 'linear'):

        self.inputs = 0
        self.z = 0 # output of the layer before activtion function
        self.weights = 1 * np.random.randn(inputs, n) / np.sqrt(inputs)
        print(np.max(self.weights))
        self.biass = np.zeros((n))


        self.set_acti(activtion_function.lower())

    def set_acti(self,activtion_function):

        if activtion_function == 'sigmoid':
            self.activtion_function = self.sigmoid
            self.d_activtion_function = self.sigmoid_derivative

        elif activtion_function == 'soft_max':
            self.activtion_function = self.soft_max
            self.d_activtion_function = self.soft_max_derivative

        elif activtion_function == 'linear':
            self.activtion_function = self.linear
            self.d_activtion_function = self.linear_derivative

        elif activtion_function == 'relu':
            self.activtion_function = self.relu
            self.d_activtion_function = self.relu_derivative

        elif activtion_function == 'tanh':
            self.activtion_function = self.tanh
            self.d_activtion_function = self.tanh_derivative


    def get_z(self):
        return self.z

    def setW(self, W):
        if self.weights.shape == W.shape:
            self.weights = W
        else:
            raise ValueError('shapes must be the same!')

    def setb(self, b):
        if self.biass.shape == b.shape:
            self.biass = b
        else:
            raise ValueError('shapes must be the same!')

    def tanh(self,x):
        return np.tanh(x)

    def tanh_derivative(self,x):
        return 1 - self.tanh(x)

    def relu(self,x):
        return x * (x >= 0)

    def relu_derivative(self,x,pram = 0):
        return np.maximum(0,x)

    # calculate the softmax of a vector
    def soft_max(self,x):
        #print('x',x)
        x -= np.max(x)
        exp_scores = np.exp(x)
        #print(exp_scores.shape)
        softmax = exp_scores / np.sum(exp_scores,
                                      axis = 1, keepdims = True)

        return softmax


    def soft_max_derivative(self,x,y,parm = 0):
        #http://saitcelebi.com/tut/output/part2.html
        #x is the input

        a = self.soft_max(x.T) # output

        #print(a.shape)
        #print(x.shape)
        da = (-y / a)
        dz = 0

        k = np.shape(a)[1]
        matrix_l = []
        for i in range(np.shape(a)[0]):
            matrix = np.dot(np.array([a[i]]).T, np.ones((1, k))) * (np.identity(k) - np.dot(np.ones((k, 1)),
                                                                                            np.array([a[i]])))
            dz = np.dot(matrix, da[i])

            matrix_l.append(dz)

        #print(matrix_l)
        matrix_l = np.array(matrix_l)
        #print(matrix_l)
        #print(matrix_l.shape)
        #print('ddd')
        return matrix_l

    def linear(self,x):
        return x*1


    def linear_derivative(self,x,parm = 0):
        return x*0 + 1

    # The Sigmoid function, which describes an S shaped curve.
    # We pass the weighted sum of the inputs through this function to
    # normalise them between 0 and 1.
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))


    def sigmoid_derivative(self,x,parm = 0):
        pass

    # The neural network thinks.
    def think(self, inputs):
        self.inputs = inputs
        self.z = np.dot(inputs, self.weights) + self.biass
        #print(self.activtion_function)
        self.output = self.activtion_function(self.z)
        # Pass inputs through our neural network (our single neuron).
        return self.output

    def think2(self,inputs):
        #print(self.activtion_function)
        self.output = self.activtion_function(np.dot(inputs, self.weights) + self.biass)
        # Pass inputs through our neural network (our single neuron).
        return self.output


class Model():
    def __init__(self):
        self.layers = []
        self.loss = None
        self.d_loss = None

        self.lr = 0.01

    def add(self, layer):
        self.layers.append(layer)


    def train(self, x, y, x_val, y_val, epochs=100, batch_size = 4):
        end = 0
        start = 0


        #set the firs values for momentum
        Mdw1,Mdb1 = 0,0
        Mdw2,Mdb2 = 0,0
        Mdw3,Mdb3 = 0,0

        #set the first values for RPSProp
        Vdw1,Vdb1 = 0,0
        Vdw2,Vdb2 = 0,0
        Vdw3,Vdb3 = 0,0

        setps = x.shape[0] // batch_size
        if setps * batch_size < x.shape[0]:
            setps+=1

        for e in range(epochs):
            shuffler = np.random.permutation(len(y))
            xs = x[shuffler]
            ys = y[shuffler]
            for step in range(setps):
                x_batch = xs[step*batch_size:(step+1) * batch_size]
                y_batch = ys[step*batch_size:(step+1) * batch_size]

                output = self.predict(x_batch)
                end = time.time()
                print(step)
                if e %5 == 0 and step >= setps-1:
                    acc = self.evaluate(x_batch,y_batch)
                    acc_val = self.evaluate(x_val,y_val)
                    #acc = None
                    #acc_val = None
                    print(f'epochs:{e},loss:{self.loss(pred=output, true=y_batch)},'
                          f'acc:{round(acc*100,2)}%,val acc:{round(acc_val*100,2)}%,'
                          f'time took{round(start-end,2)} sec')
                    start = time.time()


                #getting the weights and biass
                W1 = self.layers[0].weights
                b1 = self.layers[0].biass
                W2 = self.layers[1].weights
                b2 = self.layers[1].biass
                W3 = self.layers[2].weights
                b3 = self.layers[2].biass

                z1 = self.layers[0].z #output of the first layer before activtion function
                a1 = self.layers[1].inputs #input of the second layer(output of the first)

                z2 = self.layers[1].z #output of the first layer before activtion function
                a2 = self.layers[2].inputs #input of the second layer(output of the first)

                delta3 = np.copy(output)

                delta3[range(x_batch.shape[0]),y_batch] -= 1 #derivative of loss function(CrossEntropy)
                dW3 = a2.T.dot(delta3)
                db3 = np.sum(delta3, axis=0, keepdims=False)

                delta2 = delta3.dot(W3.T) * self.layers[1].d_activtion_function(z2)
                dW2 = np.dot(a1.T,delta2)
                db2 = np.sum(delta2, axis=0 ,keepdims=False)

                delta1 = delta2.dot(W2.T) * self.layers[0].d_activtion_function(z1)
                dW1 = np.dot(x_batch.T,delta1)
                db1 = np.sum(delta1, axis=0 ,keepdims=False)

                #momentum weights
                Mdw1 = self.beta1 * Mdw1 + (1 - self.beta1) * dW1
                Mdw2 = self.beta1 * Mdw2 + (1 - self.beta1) * dW2
                Mdw3 = self.beta1 * Mdw3 + (1 - self.beta1) * dW3

                #momentum biass
                Mdb1 = self.beta1 * Mdb1 + (1 - self.beta1) * db1
                Mdb2 = self.beta1 * Mdb2 + (1 - self.beta1) * db2
                Mdb3 = self.beta1 * Mdb3 + (1 - self.beta1) * db3

                #RPSProp weights
                Vdw1 = self.beta2 * Vdw1 + (1 - self.beta2) * np.square(dW1)
                Vdw2 = self.beta2 * Vdw2 + (1 - self.beta2) * np.square(dW2)
                Vdw3 = self.beta2 * Vdw3 + (1 - self.beta2) * np.square(dW3)

                #RPSProp biass
                Vdb1 = self.beta2 * Vdb1 + (1 - self.beta2) * np.square(db1)
                Vdb2 = self.beta2 * Vdb2 + (1 - self.beta2) * np.square(db2)
                Vdb3 = self.beta2 * Vdb3 + (1 - self.beta2) * np.square(db3)

                W1 -= (Mdw1 * self.lr) / (np.sqrt(Vdw1) + self.epsilon)
                b1 -= (Mdb1 * self.lr) / (np.sqrt(Vdb1) + self.epsilon)

                W2 -= (Mdw2 * self.lr) / (np.sqrt(Vdw2) + self.epsilon)
                b2 -= (Mdb2 * self.lr) / (np.sqrt(Vdb2) + self.epsilon)

                W3 -= (Mdw3 * self.lr) / (np.sqrt(Vdw3) + self.epsilon)
                b3 -= (Mdb3 * self.lr) / (np.sqrt(Vdb3) + self.epsilon)


                self.layers[0].setW(W1)
                self.layers[0].setb(b1)

                self.layers[1].setW(W2)
                self.layers[1].setb(b2)

                self.layers[2].setW(W3)
                self.layers[2].setb(b3)



    def mse(self, pred, true):
        return np.mean((true - pred) ** 2)


    def mse_derivative(self, pred,true):
        #TODO: add derivative by biass!
        return -2*(true - pred)


    def CrossEntropy(self,pred, true):
        m = np.zeros(true.shape[0])
        for i,correct_index in enumerate(true):
            predicted = pred[i][correct_index]
            m[i] = predicted
        log_prob = -np.log(m)
        loss = np.sum(log_prob)

        return float(loss / true.shape[0])


    def CrossEntropy_derivative(self,pred,true):

        # Number of samples
        pred = np.clip(pred,np.min(pred)+1e-50,np.max(pred)+1e-50)

        samples = len(pred)
        # Number of labels in every sample
        # We'll use the first sample to count them
        labels = len(pred[0])
        # If labels are sparse, turn them into one-hot vector
        y_true = true
        if len(true.shape) == 1:
            y_true = np.eye(labels)[true]
        # Calculate gradient
        dinputs = -y_true / pred
        # Normalize gradient
        dinputs = dinputs / samples
        return dinputs



    def compile(self,loss, lr = 0.0001, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-7):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon

        if loss == 'mse':
            self.loss = self.mse
            self.d_loss = self.mse_derivative

        if loss == 'crossEntropy':
            self.loss = self.CrossEntropy
            self.d_loss = self.CrossEntropy_derivative


    def predict(self, x):
        t = x
        for lay in self.layers:
            t = lay.think(t)

        return t

    def predict2(self,x):
        t = x
        for lay in self.layers:
            t = lay.think2(t)

        return t

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))


    def sigmoid_derivative(self, x):
        return self.sigmoid(x) * (1 - self.sigmoid(x))

    # calculate the softmax of a vector
    def soft_max(self,x):
        e = np.exp(x)
        d = np.sum(e,axis=1)
        g = np.array(np.split(d,list(range(0,d.shape[0])))[1:])
        raise '????'
        #return e / g



    def evaluate(self, X_test, y_test):
        y = np.copy(y_test)
        if y.shape[0] != 1:
            y = np.reshape(y,(1,y.shape[0]))
        x = np.copy(X_test)
        pred = self.predict2(x)
        pred = np.argmax(pred,axis=1)
        score = np.argwhere(pred == y[0])
        return score.shape[0] / y.shape[1]


    def save(self, path):
        W = []
        b = []
        for lay in self.layers:
            W.append(lay.weights)
            b.append(lay.biass)

        prameters = {'w':W,'b':b}
        pickle.dump(prameters, open(path + '.p', 'wb'))


    def load(self, path):
        prameters = pickle.load(open(path, 'rb'))
        W = prameters['w']
        B = prameters['b']

        for i,wb in enumerate(zip(W,B)):
            self.layers[i].setW(wb[0])
            self.layers[i].setb(wb[1])




if __name__ == '__main__':
    X_train =  np.array([[0,1,1],
                   [1,1,1],
                   [0,1,0],
                   [1,0,1]])

    y_train = np.array([0,1,0,1])
    print(y_train.shape,'y')



    X_train, X_test, y_train, y_test = HD.get_data(r'D:\cyber\yb project\databases\photos\test',
                                                   val= 0.3,n = 4, j = -1)
    X_train,X_test = HD.scale_data(X_train,X_test)

    print(y_train.shape)
    print(y_train)
    y_train = y_train[0]

    print(X_train.shape,y_train.shape)

    print('data loaded!')
    print(X_test[0].shape)
    t = np.reshape(X_test[0],(128,128))
    #t *= 255

    print(np.max(y_train)+1)
    plt.imshow(t)
    plt.show()

    layer1 = Danse(n=16384//2, inputs=16384,activtion_function = 'relu')
    layer2 = Danse(n=256, inputs=16384//2,activtion_function = 'relu')
    layer3 = Danse(n=np.max(y_train)+1, inputs=256,activtion_function = 'soft_max')
    model = Model()


    model.add(layer1)
    model.add(layer2)
    model.add(layer3)

    #model.load(r'C:\Users\Dvir hamdi\PycharmProjects\cyberHW\yodbet project\model.p')
    model.compile(loss='crossEntropy',lr = 0.00001)
    model.train(X_train, y_train,X_test,y_test, epochs=100,batch_size=2048)

    print('model trained!')
    #os.chdir(r'C:\Users\Dvir hamdi\PycharmProjects\cyberHW\yodbet project')
    print('pred',model.predict(X_test))

    model.save(r'C:\Users\Dvir hamdi\PycharmProjects\cyberHW\yodbet project\model')
    #model.load('model.p')
    # Test the neural network with a new situations.
    print('acc:', model.evaluate(X_test, y_test) * 100, '%')

    print(y_test[0])
    plt.imshow(np.reshape(X_test[0],(128,128)))
    plt.show()

