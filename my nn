import numpy as np

class Danse:
    def __init__(self,n,inputs):
        self.inputs = inputs
        self.weights = np.random.normal(0,0.25,(n,inputs))
        self.biases = np.zeros((1,n))

        print('weights:',self.weights.shape)
        print('biases:',self.biases)

    def culculate(self,x):
        return np.dot(x,self.weights.T) + self.biases

    # Backward pass
    def backward(self, dvalues):
        # Gradients on parameters
        self.dweights = np.dot(self.inputs.T, dvalues)
        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)

        # Gradient on values
        self.dinputs = np.dot(dvalues, self.weights.T)

class Activation_Sigmoid:

    # Forward pass
    def culculate(self, inputs):
        # Save input and calculate/save output
        # of the sigmoid function
        self.inputs = inputs
        self.output = 1 / (1 + np.exp(-inputs))

        return self.output

    # Backward pass
    def backward(self, dvalues):
        # Derivative - calculates from output of the sigmoid function
        self.dinputs = dvalues * (1 - self.output) * self.output


# Binary cross-entropy loss
class Loss_BinaryCrossentropy:
    # Forward pass
    def culculate(self, y_pred, y_true):
        # Clip data to prevent division by 0
        # Clip both sides to not drag mean towards any value
        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)

        # Calculate sample-wise loss
        sample_losses = -(y_true * np.log(y_pred_clipped) +
                          (1 - y_true) * np.log(1 - y_pred_clipped))
        sample_losses = np.mean(sample_losses, axis=-1)

        # Return losses
        return sample_losses


    # Backward pass
    def backward(self, dvalues, y_true):
        # Number of samples
        samples = len(dvalues)

        # Number of outputs in every sample
        # We'll use the first sample to count them
        outputs = len(dvalues[0])

        # Clip data to prevent division by 0
        # Clip both sides to not drag mean towards any value
        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)

        # Calculate gradient
        self.dinputs = -(y_true / clipped_dvalues -
                         (1 - y_true) / (1 - clipped_dvalues)) / outputs

        # Normalize gradient
        self.dinputs = self.dinputs / samples


class Adam_Optimizer:

    def __init__(self,lr):
        self.lr = lr

    def get_parms(self):
        return self.lr

class Model:
    def __init__(self):
        self.layers = []

    def Add_Danse(self,n,inputs):
        self.layers.append(Danse(n,inputs))

    def Add_layer(self,layer):
        self.layers.append(layer)

    def prdict(self,x):
        t = x
        for lay in self.layers:
            t = lay.culculate(t)

        return t

    def train(self,x,y,epochs = 5):
        pass


    def compile(self,optimizer,loss_func):
        self.optimizer = optimizer
        self.loss_func = loss_func



#np.random.seed(1)

x = [[1,1,1],
     [1,0,1],
     [1,0,0]]

y = np.array([1,1,1])

model = Model()
model.Add_Danse(1,3)
model.Add_layer(Activation_Sigmoid())
model.Add_Danse(1,1)
model.Add_layer(Activation_Sigmoid())

out = model.prdict(x)

loss = Loss_BinaryCrossentropy()
loss_res = loss.culculate(out,y)

print('loss:',np.mean(loss_res))
print(out)
