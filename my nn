import numpy as np

class Danse:
    def __init__(self,n,inputs):
        #self.inputs = inputs
        self.weights = np.random.normal(0,0.25,(n,inputs))
        self.biases = np.zeros((1,n))

        print('weights:',self.weights.shape)
        print('biases:',self.biases)

    def culculate(self,x):
        self.inputs = x
        return np.dot(x,self.weights) + self.biases

    # Backward pass
    def backward(self, dvalues):
        print('dddddddddgg!!!',self.weights,dvalues)
        # Gradients on parameters
        self.dweights = np.dot(self.inputs.T, dvalues)
        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)

        # Gradient on values
        self.dinputs = np.dot(dvalues, self.weights)

        return self.dinputs

    def GetW(self):
        return self.weights

    def GetB(self):
        return self.biases

    def GetDW(self):
        return self.dweights

    def GetDB(self):
        return self.dbiases

class Activation_Sigmoid:

    # Forward pass
    def culculate(self, inputs):
        # Save input and calculate/save output
        # of the sigmoid function
        self.inputs = inputs
        self.output = 1 / (1 + np.exp(-inputs))

        return self.output

    # Backward pass
    def backward(self, dvalues):
        # Derivative - calculates from output of the sigmoid function
        self.dinputs = dvalues * (1 - self.output) * self.output
        return self.dinputs

# Binary cross-entropy loss
class Loss_BinaryCrossentropy:
    # Forward pass
    def culculate(self, y_pred, y_true):
        # Clip data to prevent division by 0
        # Clip both sides to not drag mean towards any value
        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)

        # Calculate sample-wise loss
        sample_losses = -(y_true * np.log(y_pred_clipped) +
                          (1 - y_true) * np.log(1 - y_pred_clipped))
        sample_losses = np.mean(sample_losses, axis=-1)

        # Return losses
        return sample_losses


    # Backward pass
    def backward(self, dvalues, y_true):
        # Number of samples
        samples = len(dvalues)

        # Number of outputs in every sample
        # We'll use the first sample to count them
        outputs = len(dvalues[0])

        # Clip data to prevent division by 0
        # Clip both sides to not drag mean towards any value
        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)

        # Calculate gradient
        self.dinputs = -(y_true / clipped_dvalues -
                         (1 - y_true) / (1 - clipped_dvalues)) / outputs

        # Normalize gradient
        self.dinputs = self.dinputs / samples

        return self.dinputs

class Adam_Optimizer:

    def __init__(self,lr):
        self.lr = lr

    def get_parms(self):
        return self.lr

class Model:
    def __init__(self):
        self.layers = []

    def Add_Danse(self,n,inputs):
        self.layers.append(Danse(n,inputs))

    def Add_layer(self,layer):
        self.layers.append(layer)

    def prdict(self,x):
        t = x
        for lay in self.layers:
            t = lay.culculate(t)

        return t

    def train(self,x,y,epochs = 5):
        for e in range(epochs):
            self.ChangeWB(x,y)

    def ChangeWB(self,x,y):
        y_pred = self.prdict(x)
        dvalues = self.loss_func.backward(y_pred,y)

        #loss = self.a.culculate(x)

        for lay in reversed(self.layers):
            dvalues = lay.backward(dvalues)

            try:
                o_w = lay.GetW()
                #o_w -= lay.GetDW()*self.optimizer.lr what shuld be written
                print(lay.GetDW().shape,'ggggggggggggggggggggggggg')
                o_w -= lay.GetDW()*0.1

            except AttributeError:
                pass


    def compile(self,loss_func,optimizer = None):
        self.optimizer = optimizer
        self.loss_func = loss_func



#np.random.seed(1)

x = np.array([[1,1,1],
     [1,0,1],
     [1,0,0]])

y = np.array([1,1,1])

model = Model()
model.Add_Danse(3,3)
model.Add_layer(Activation_Sigmoid())
model.Add_Danse(3,1)
model.Add_layer(Activation_Sigmoid())

model.compile(Loss_BinaryCrossentropy())
model.train(x,y)

out = model.prdict(x)

loss = Loss_BinaryCrossentropy()
loss_res = loss.culculate(out,y)

print('loss:',np.mean(loss_res))
print(out)
